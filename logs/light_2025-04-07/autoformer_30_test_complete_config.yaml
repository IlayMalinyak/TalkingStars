{
  "light_transforms": "Compose(\n    moving_avg(kernel_size=13, stride=1)\n    ACF(max_lag=None)\n    Normalize(scheme='['std']', axis=0)\n    ToTensor\n)",
  "spec_transforms": "None",
  "train_dataset": "<dataset.dataset.KeplerINRDataset object at 0x7f8debb7be00>",
  "val_dataset": "<dataset.dataset.KeplerINRDataset object at 0x7f8de8c57200>",
  "test_dataset": "<dataset.dataset.KeplerINRDataset object at 0x7f8de8b35970>",
  "model_args": "<util.utils.Container object at 0x7f8df194fd10>",
  "model": "DistributedDataParallel(\n  (module): INREncoderDecoder(\n    (encoder_decoder): Model(\n      (decomp): series_decomp(\n        (moving_avg): moving_avg(\n          (avg): AvgPool1d(kernel_size=(2401,), stride=(1,), padding=(1200,))\n        )\n      )\n      (enc_embedding): DataEmbedding_wo_pos(\n        (value_embedding): TokenEmbedding(\n          (tokenConv): Conv1d(1, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n        )\n        (position_embedding): PositionalEmbedding()\n        (temporal_embedding): TimeFeatureEmbedding(\n          (embed): Linear(in_features=1, out_features=64, bias=False)\n        )\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (dec_embedding): DataEmbedding_wo_pos(\n        (value_embedding): TokenEmbedding(\n          (tokenConv): Conv1d(1, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n        )\n        (position_embedding): PositionalEmbedding()\n        (temporal_embedding): TimeFeatureEmbedding(\n          (embed): Linear(in_features=1, out_features=64, bias=False)\n        )\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (encoder): Encoder(\n        (attn_layers): ModuleList(\n          (0-1): 2 x EncoderLayer(\n            (attention): AutoCorrelationLayer(\n              (inner_correlation): AutoCorrelation(\n                (dropout): Dropout(p=0.2, inplace=False)\n              )\n              (query_projection): Linear(in_features=64, out_features=64, bias=True)\n              (key_projection): Linear(in_features=64, out_features=64, bias=True)\n              (value_projection): Linear(in_features=64, out_features=64, bias=True)\n              (out_projection): Linear(in_features=64, out_features=64, bias=True)\n            )\n            (conv1): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n            (conv2): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n            (decomp1): series_decomp(\n              (moving_avg): moving_avg(\n                (avg): AvgPool1d(kernel_size=(2401,), stride=(1,), padding=(1200,))\n              )\n            )\n            (decomp2): series_decomp(\n              (moving_avg): moving_avg(\n                (avg): AvgPool1d(kernel_size=(2401,), stride=(1,), padding=(1200,))\n              )\n            )\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n        )\n        (norm): my_Layernorm(\n          (layernorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (decoder): Decoder(\n        (layers): ModuleList(\n          (0-1): 2 x DecoderLayer(\n            (self_attention): AutoCorrelationLayer(\n              (inner_correlation): AutoCorrelation(\n                (dropout): Dropout(p=0.2, inplace=False)\n              )\n              (query_projection): Linear(in_features=64, out_features=64, bias=True)\n              (key_projection): Linear(in_features=64, out_features=64, bias=True)\n              (value_projection): Linear(in_features=64, out_features=64, bias=True)\n              (out_projection): Linear(in_features=64, out_features=64, bias=True)\n            )\n            (cross_attention): AutoCorrelationLayer(\n              (inner_correlation): AutoCorrelation(\n                (dropout): Dropout(p=0.2, inplace=False)\n              )\n              (query_projection): Linear(in_features=64, out_features=64, bias=True)\n              (key_projection): Linear(in_features=64, out_features=64, bias=True)\n              (value_projection): Linear(in_features=64, out_features=64, bias=True)\n              (out_projection): Linear(in_features=64, out_features=64, bias=True)\n            )\n            (conv1): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n            (conv2): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n            (decomp1): series_decomp(\n              (moving_avg): moving_avg(\n                (avg): AvgPool1d(kernel_size=(2401,), stride=(1,), padding=(1200,))\n              )\n            )\n            (decomp2): series_decomp(\n              (moving_avg): moving_avg(\n                (avg): AvgPool1d(kernel_size=(2401,), stride=(1,), padding=(1200,))\n              )\n            )\n            (decomp3): series_decomp(\n              (moving_avg): moving_avg(\n                (avg): AvgPool1d(kernel_size=(2401,), stride=(1,), padding=(1200,))\n              )\n            )\n            (dropout): Dropout(p=0.2, inplace=False)\n            (projection): Conv1d(64, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n          )\n        )\n        (norm): my_Layernorm(\n          (layernorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        )\n        (projection): Linear(in_features=64, out_features=1, bias=True)\n      )\n      (head): Sequential(\n        (0): Linear(in_features=64, out_features=16, bias=True)\n        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): GELU(approximate='none')\n        (3): Dropout(p=0.2, inplace=False)\n        (4): Linear(in_features=16, out_features=1, bias=True)\n      )\n    )\n    (inr_model): RelationalTransformer(\n      (construct_graph): GraphConstructor(\n        (proj_weight): Sequential(\n          (0): Linear(in_features=1, out_features=32, bias=True)\n        )\n        (proj_bias): Sequential(\n          (0): Linear(in_features=1, out_features=32, bias=True)\n        )\n        (proj_node_in): Linear(in_features=32, out_features=32, bias=True)\n        (proj_edge_in): Linear(in_features=32, out_features=32, bias=True)\n      )\n      (layers): ModuleList(\n        (0-3): 4 x RecursiveScriptModule(\n          original_name=RTLayer\n          (self_attn): RecursiveScriptModule(\n            original_name=RTAttention\n            (split_head_node): RecursiveScriptModule(original_name=Rearrange)\n            (split_head_edge): RecursiveScriptModule(original_name=Rearrange)\n            (cat_head_node): RecursiveScriptModule(original_name=Rearrange)\n            (qkv_node): RecursiveScriptModule(original_name=Linear)\n            (qkv_edge): RecursiveScriptModule(original_name=Linear)\n            (proj_out): RecursiveScriptModule(original_name=Linear)\n          )\n          (lin0): RecursiveScriptModule(original_name=Linear)\n          (dropout0): RecursiveScriptModule(original_name=Dropout)\n          (node_ln0): RecursiveScriptModule(original_name=LayerNorm)\n          (node_ln1): RecursiveScriptModule(original_name=LayerNorm)\n          (node_mlp): RecursiveScriptModule(\n            original_name=Sequential\n            (0): RecursiveScriptModule(original_name=Linear)\n            (1): RecursiveScriptModule(original_name=GELU)\n            (2): RecursiveScriptModule(original_name=Linear)\n            (3): RecursiveScriptModule(original_name=Dropout)\n          )\n          (edge_updates): RecursiveScriptModule(\n            original_name=EdgeLayer\n            (edge_mlp0): RecursiveScriptModule(\n              original_name=EdgeMLP\n              (reverse_edge): RecursiveScriptModule(original_name=Rearrange)\n              (lin0_e): RecursiveScriptModule(original_name=Linear)\n              (lin0_s): RecursiveScriptModule(original_name=Linear)\n              (lin0_t): RecursiveScriptModule(original_name=Linear)\n              (act): RecursiveScriptModule(original_name=GELU)\n              (lin1): RecursiveScriptModule(original_name=Linear)\n              (drop): RecursiveScriptModule(original_name=Dropout)\n            )\n            (edge_mlp1): RecursiveScriptModule(\n              original_name=Sequential\n              (0): RecursiveScriptModule(original_name=Linear)\n              (1): RecursiveScriptModule(original_name=GELU)\n              (2): RecursiveScriptModule(original_name=Linear)\n              (3): RecursiveScriptModule(original_name=Dropout)\n            )\n            (eln0): RecursiveScriptModule(original_name=LayerNorm)\n            (eln1): RecursiveScriptModule(original_name=LayerNorm)\n          )\n        )\n      )\n      (pool): AttentionBasedPooling(\n        (gmt): GraphMultisetTransformer(32, k=32, heads=8, layer_norm=False, dropout=0.0)\n      )\n      (proj_out): Sequential(\n        (0): Linear(in_features=32, out_features=32, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=32, out_features=32, bias=True)\n      )\n    )\n  )\n)",
  "trainer": {
    "model": "DistributedDataParallel(\n  (module): INREncoderDecoder(\n    (encoder_decoder): Model(\n      (decomp): series_decomp(\n        (moving_avg): moving_avg(\n          (avg): AvgPool1d(kernel_size=(2401,), stride=(1,), padding=(1200,))\n        )\n      )\n      (enc_embedding): DataEmbedding_wo_pos(\n        (value_embedding): TokenEmbedding(\n          (tokenConv): Conv1d(1, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n        )\n        (position_embedding): PositionalEmbedding()\n        (temporal_embedding): TimeFeatureEmbedding(\n          (embed): Linear(in_features=1, out_features=64, bias=False)\n        )\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (dec_embedding): DataEmbedding_wo_pos(\n        (value_embedding): TokenEmbedding(\n          (tokenConv): Conv1d(1, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n        )\n        (position_embedding): PositionalEmbedding()\n        (temporal_embedding): TimeFeatureEmbedding(\n          (embed): Linear(in_features=1, out_features=64, bias=False)\n        )\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (encoder): Encoder(\n        (attn_layers): ModuleList(\n          (0-1): 2 x EncoderLayer(\n            (attention): AutoCorrelationLayer(\n              (inner_correlation): AutoCorrelation(\n                (dropout): Dropout(p=0.2, inplace=False)\n              )\n              (query_projection): Linear(in_features=64, out_features=64, bias=True)\n              (key_projection): Linear(in_features=64, out_features=64, bias=True)\n              (value_projection): Linear(in_features=64, out_features=64, bias=True)\n              (out_projection): Linear(in_features=64, out_features=64, bias=True)\n            )\n            (conv1): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n            (conv2): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n            (decomp1): series_decomp(\n              (moving_avg): moving_avg(\n                (avg): AvgPool1d(kernel_size=(2401,), stride=(1,), padding=(1200,))\n              )\n            )\n            (decomp2): series_decomp(\n              (moving_avg): moving_avg(\n                (avg): AvgPool1d(kernel_size=(2401,), stride=(1,), padding=(1200,))\n              )\n            )\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n        )\n        (norm): my_Layernorm(\n          (layernorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (decoder): Decoder(\n        (layers): ModuleList(\n          (0-1): 2 x DecoderLayer(\n            (self_attention): AutoCorrelationLayer(\n              (inner_correlation): AutoCorrelation(\n                (dropout): Dropout(p=0.2, inplace=False)\n              )\n              (query_projection): Linear(in_features=64, out_features=64, bias=True)\n              (key_projection): Linear(in_features=64, out_features=64, bias=True)\n              (value_projection): Linear(in_features=64, out_features=64, bias=True)\n              (out_projection): Linear(in_features=64, out_features=64, bias=True)\n            )\n            (cross_attention): AutoCorrelationLayer(\n              (inner_correlation): AutoCorrelation(\n                (dropout): Dropout(p=0.2, inplace=False)\n              )\n              (query_projection): Linear(in_features=64, out_features=64, bias=True)\n              (key_projection): Linear(in_features=64, out_features=64, bias=True)\n              (value_projection): Linear(in_features=64, out_features=64, bias=True)\n              (out_projection): Linear(in_features=64, out_features=64, bias=True)\n            )\n            (conv1): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n            (conv2): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n            (decomp1): series_decomp(\n              (moving_avg): moving_avg(\n                (avg): AvgPool1d(kernel_size=(2401,), stride=(1,), padding=(1200,))\n              )\n            )\n            (decomp2): series_decomp(\n              (moving_avg): moving_avg(\n                (avg): AvgPool1d(kernel_size=(2401,), stride=(1,), padding=(1200,))\n              )\n            )\n            (decomp3): series_decomp(\n              (moving_avg): moving_avg(\n                (avg): AvgPool1d(kernel_size=(2401,), stride=(1,), padding=(1200,))\n              )\n            )\n            (dropout): Dropout(p=0.2, inplace=False)\n            (projection): Conv1d(64, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n          )\n        )\n        (norm): my_Layernorm(\n          (layernorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        )\n        (projection): Linear(in_features=64, out_features=1, bias=True)\n      )\n      (head): Sequential(\n        (0): Linear(in_features=64, out_features=16, bias=True)\n        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): GELU(approximate='none')\n        (3): Dropout(p=0.2, inplace=False)\n        (4): Linear(in_features=16, out_features=1, bias=True)\n      )\n    )\n    (inr_model): RelationalTransformer(\n      (construct_graph): GraphConstructor(\n        (proj_weight): Sequential(\n          (0): Linear(in_features=1, out_features=32, bias=True)\n        )\n        (proj_bias): Sequential(\n          (0): Linear(in_features=1, out_features=32, bias=True)\n        )\n        (proj_node_in): Linear(in_features=32, out_features=32, bias=True)\n        (proj_edge_in): Linear(in_features=32, out_features=32, bias=True)\n      )\n      (layers): ModuleList(\n        (0-3): 4 x RecursiveScriptModule(\n          original_name=RTLayer\n          (self_attn): RecursiveScriptModule(\n            original_name=RTAttention\n            (split_head_node): RecursiveScriptModule(original_name=Rearrange)\n            (split_head_edge): RecursiveScriptModule(original_name=Rearrange)\n            (cat_head_node): RecursiveScriptModule(original_name=Rearrange)\n            (qkv_node): RecursiveScriptModule(original_name=Linear)\n            (qkv_edge): RecursiveScriptModule(original_name=Linear)\n            (proj_out): RecursiveScriptModule(original_name=Linear)\n          )\n          (lin0): RecursiveScriptModule(original_name=Linear)\n          (dropout0): RecursiveScriptModule(original_name=Dropout)\n          (node_ln0): RecursiveScriptModule(original_name=LayerNorm)\n          (node_ln1): RecursiveScriptModule(original_name=LayerNorm)\n          (node_mlp): RecursiveScriptModule(\n            original_name=Sequential\n            (0): RecursiveScriptModule(original_name=Linear)\n            (1): RecursiveScriptModule(original_name=GELU)\n            (2): RecursiveScriptModule(original_name=Linear)\n            (3): RecursiveScriptModule(original_name=Dropout)\n          )\n          (edge_updates): RecursiveScriptModule(\n            original_name=EdgeLayer\n            (edge_mlp0): RecursiveScriptModule(\n              original_name=EdgeMLP\n              (reverse_edge): RecursiveScriptModule(original_name=Rearrange)\n              (lin0_e): RecursiveScriptModule(original_name=Linear)\n              (lin0_s): RecursiveScriptModule(original_name=Linear)\n              (lin0_t): RecursiveScriptModule(original_name=Linear)\n              (act): RecursiveScriptModule(original_name=GELU)\n              (lin1): RecursiveScriptModule(original_name=Linear)\n              (drop): RecursiveScriptModule(original_name=Dropout)\n            )\n            (edge_mlp1): RecursiveScriptModule(\n              original_name=Sequential\n              (0): RecursiveScriptModule(original_name=Linear)\n              (1): RecursiveScriptModule(original_name=GELU)\n              (2): RecursiveScriptModule(original_name=Linear)\n              (3): RecursiveScriptModule(original_name=Dropout)\n            )\n            (eln0): RecursiveScriptModule(original_name=LayerNorm)\n            (eln1): RecursiveScriptModule(original_name=LayerNorm)\n          )\n        )\n      )\n      (pool): AttentionBasedPooling(\n        (gmt): GraphMultisetTransformer(32, k=32, heads=8, layer_norm=False, dropout=0.0)\n      )\n      (proj_out): Sequential(\n        (0): Linear(in_features=32, out_features=32, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=32, out_features=32, bias=True)\n      )\n    )\n  )\n)",
    "optimizer": "AdamW (\nParameter Group 0\n    amsgrad: False\n    base_momentum: 0.85\n    betas: (0.95, 0.999)\n    capturable: False\n    decoupled_weight_decay: True\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    initial_lr: 1e-05\n    lr: 9.999999999999999e-06\n    max_lr: 0.0001\n    max_momentum: 0.95\n    maximize: False\n    min_lr: 1.0000000000000001e-07\n    weight_decay: 1e-06\n)",
    "criterion": "L1Loss()",
    "scaler": "<torch.cuda.amp.grad_scaler.GradScaler object at 0x7f8de6dbd310>",
    "grad_clip": false,
    "cos_inc": false,
    "output_dim": 1,
    "scheduler": "<torch.optim.lr_scheduler.OneCycleLR object at 0x7f8de6dbfb30>",
    "train_dl": "<torch.utils.data.dataloader.DataLoader object at 0x7f8de8af30e0>",
    "val_dl": "<torch.utils.data.dataloader.DataLoader object at 0x7f8deb7b0d70>",
    "train_sampler": "<torch.utils.data.distributed.DistributedSampler object at 0x7f8e067b8530>",
    "val_sampler": "<torch.utils.data.distributed.DistributedSampler object at 0x7f8de6d12c00>",
    "max_iter": 50,
    "device": 0,
    "world_size": 1,
    "exp_num": "light_2025-04-07",
    "exp_name": "lc_autoformer_30_test",
    "log_path": "/data/TalkingStars/logs",
    "best_state_dict": null,
    "plot_every": null,
    "logger": null,
    "range_update": null,
    "accumulation_step": 1,
    "wandb": false,
    "num_quantiles": 1,
    "update_func": "<function Trainer.<lambda> at 0x7f8df26382c0>",
    "pred_len": 1440,
    "label_len": 1440,
    "use_inr": true
  },
  "loss_fn": "L1Loss()",
  "optimizer": "AdamW (\nParameter Group 0\n    amsgrad: False\n    base_momentum: 0.85\n    betas: (0.95, 0.999)\n    capturable: False\n    decoupled_weight_decay: True\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    initial_lr: 1e-05\n    lr: 9.999999999999999e-06\n    max_lr: 0.0001\n    max_momentum: 0.95\n    maximize: False\n    min_lr: 1.0000000000000001e-07\n    weight_decay: 1e-06\n)"
}